import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, auc
)
import torch.nn.functional as F      # –ù—É–∂–Ω–æ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (Softmax)
from metrics_utils import ModelEvaluator # –ù–∞—à –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å

class ModelEvaluator:
    def __init__(self, save_dir="evaluation_results"):
        """
        –ö–ª–∞—Å—Å –¥–ª—è —Å–±–æ—Ä–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, –ø–æ–¥—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –∏ —Ä–∏—Å–æ–≤–∞–Ω–∏—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤.
        """
        self.save_dir = save_dir
        os.makedirs(self.save_dir, exist_ok=True)
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: { 'Model_Name': {'y_true': [], 'y_prob': [], 'metrics': {...}} }
        self.experiments = {}

    def add_predictions(self, model_name, y_true, y_prob):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏.
        y_true: –∏—Å—Ç–∏–Ω–Ω—ã–µ –ª–µ–π–±–ª—ã (0 –∏–ª–∏ 1)
        y_prob: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ 1 (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å–ª–µ softmax/sigmoid)
        """
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ numpy
        y_true = np.array(y_true)
        y_prob = np.array(y_prob)
        y_pred = (y_prob > 0.5).astype(int)

        # –°—á–∏—Ç–∞–µ–º —Å–∫–∞–ª—è—Ä–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            "Accuracy": accuracy_score(y_true, y_pred),
            "Precision": precision_score(y_true, y_pred, zero_division=0),
            "Recall": recall_score(y_true, y_pred, zero_division=0),
            "F1-Score": f1_score(y_true, y_pred, zero_division=0),
            "ROC-AUC": roc_auc_score(y_true, y_prob)
        }

        self.experiments[model_name] = {
            "y_true": y_true,
            "y_prob": y_prob,
            "y_pred": y_pred,
            "metrics": metrics
        }
        print(f"‚úÖ [Evaluator] Added {model_name}: F1={metrics['F1-Score']:.4f}, AUC={metrics['ROC-AUC']:.4f}")

    def save_metrics_to_json(self, filename="all_metrics.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ JSON –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏."""
        data_to_save = {name: data['metrics'] for name, data in self.experiments.items()}
        path = os.path.join(self.save_dir, filename)
        with open(path, 'w') as f:
            json.dump(data_to_save, f, indent=4)
        print(f"üíæ Metrics saved to {path}")

    def plot_roc_curves(self):
        """–†–∏—Å—É–µ—Ç ROC-–∫—Ä–∏–≤—ã–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º –≥—Ä–∞—Ñ–∏–∫–µ."""
        plt.figure(figsize=(10, 8))
        
        for name, data in self.experiments.items():
            fpr, tpr, _ = roc_curve(data['y_true'], data['y_prob'])
            auc_val = data['metrics']['ROC-AUC']
            plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {auc_val:.3f})')

        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve Comparison')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        
        self._save_plot("comparison_roc_curve.png")

    def plot_pr_curves(self):
        """–†–∏—Å—É–µ—Ç Precision-Recall –∫—Ä–∏–≤—ã–µ."""
        plt.figure(figsize=(10, 8))
        
        for name, data in self.experiments.items():
            precision, recall, _ = precision_recall_curve(data['y_true'], data['y_prob'])
            pr_auc = auc(recall, precision)
            plt.plot(recall, precision, lw=2, label=f'{name} (AUC = {pr_auc:.3f})')

        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve Comparison')
        plt.legend(loc="lower left")
        plt.grid(True, alpha=0.3)
        
        self._save_plot("comparison_pr_curve.png")

    def plot_confusion_matrices(self):
        """–†–∏—Å—É–µ—Ç Confusion Matrix –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ (—Å–µ—Ç–∫–æ–π)."""
        n = len(self.experiments)
        if n == 0: return

        cols = min(3, n)
        rows = (n + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))
        axes = axes.flatten() if n > 1 else [axes]

        for i, (name, data) in enumerate(self.experiments.items()):
            cm = confusion_matrix(data['y_true'], data['y_pred'])
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i], cbar=False)
            axes[i].set_title(f"{name}\nF1: {data['metrics']['F1-Score']:.3f}")
            axes[i].set_xlabel("Predicted")
            axes[i].set_ylabel("True")

        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –æ—Å–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        for j in range(i + 1, len(axes)):
            fig.delaxes(axes[j])

        plt.tight_layout()
        self._save_plot("comparison_confusion_matrices.png")

    def plot_metric_bar_chart(self):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç–æ–ª–±–∏–∫–∞–º–∏."""
        df_list = []
        for name, data in self.experiments.items():
            m = data['metrics']
            df_list.append({
                "Model": name, "Accuracy": m["Accuracy"], "F1": m["F1-Score"], "AUC": m["ROC-AUC"]
            })
        
        df = pd.DataFrame(df_list)
        df_melted = df.melt(id_vars="Model", var_name="Metric", value_name="Score")

        plt.figure(figsize=(10, 6))
        sns.barplot(data=df_melted, x="Model", y="Score", hue="Metric", palette="viridis")
        plt.title("Model Performance Comparison")
        plt.ylim(0.5, 1.0) # –û–±—ã—á–Ω–æ –º–µ—Ç—Ä–∏–∫–∏ –≤—ã—à–µ 0.5, —Ç–∞–∫ –≤–∏–¥–Ω–µ–µ —Ä–∞–∑–Ω–∏—Ü—É
        plt.grid(axis='y', alpha=0.3)
        
        self._save_plot("comparison_bar_chart.png")

    def _save_plot(self, filename):
        path = os.path.join(self.save_dir, filename)
        plt.savefig(path, dpi=150)
        plt.close()
        print(f"üñº Plot saved: {path}")
